{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('science', 2), ('fiction', 1), ('data', 2), ('big', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter,defaultdict\n",
    "import re\n",
    "\n",
    "def tokenize(message):\n",
    "    message=message.lower()\n",
    "    all_words=re.findall(\"[a-z0-9']+\", message)\n",
    "    return set(all_words)\n",
    "\n",
    "def word_count_old(doocuments):\n",
    "    return Counter(word\n",
    "                  for document in documents\n",
    "                  for word in document)\n",
    "\n",
    "def wc_mapper(document):\n",
    "    for word in tokenize(document):\n",
    "        yield (word,1)\n",
    "        \n",
    "def wc_reducer(word,counts):\n",
    "    yield (word,sum(counts))\n",
    "\n",
    "def word_count(documents):\n",
    "    collector=defaultdict(list)\n",
    "    \n",
    "    for document in documents:\n",
    "        for word,count in wc_mapper(document):\n",
    "            collector[word].append(count)\n",
    "    return [output\n",
    "           for word,counts in collector.iteritems()\n",
    "           for output in wc_reducer(word,counts)]\n",
    "\n",
    "documents=[\"data science\",\"big data\",\"science fiction\"]\n",
    "\n",
    "print word_count(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'partial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-756a877a8945>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduce_values_using\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maggregation_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0msum_reducer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalues_reducer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mmax_reducer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalues_reducer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-756a877a8945>\u001b[0m in \u001b[0;36mvalues_reducer\u001b[0;34m(aggregation_fn)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvalues_reducer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maggregation_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduce_values_using\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maggregation_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0msum_reducer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalues_reducer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'partial' is not defined"
     ]
    }
   ],
   "source": [
    "def map_reduce(inputs,mapper,reducer):\n",
    "    collector=defaultdict(list)\n",
    "    \n",
    "    for input in inputs:\n",
    "        for key,value in mapper(input):\n",
    "            collector[key].append(value)\n",
    "    return [output for key,values in collector.iteritems()\n",
    "           for output in reducer(key,values)]\n",
    "\n",
    "word_counts=map_reduce(documents,wc_mapper,wc_reducer)\n",
    "\n",
    "\n",
    "\n",
    "def reduce_values_using(aggregation_fn,key,values):\n",
    "    yield (key,aggregation_fn(values))\n",
    "    \n",
    "def values_reducer(aggregation_fn):\n",
    "    return partial(reduce_values_using,aggregation_fn)\n",
    "\n",
    "sum_reducer=values_reducer(sum)\n",
    "max_reducer=values_reducer(max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sum_reducer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-4f5e6aae83dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mday_of_week\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdata_science_days\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmap_reduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus_updates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_science_day_mapper\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msum_reducer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sum_reducer' is not defined"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "status_updates={\"id\":1,\n",
    " \"username\":\"joelgrus\",\n",
    " \"text\":\"Is anyone interested in a data science book?\",\n",
    " \"created_at\": datetime.datetime(2013,12,21,11,47),\n",
    " \"liked_by\":[\"data_guy\",\"data_girl\",\"mike\"]\n",
    "}\n",
    "\n",
    "def data_science_day_mapper(status_update):\n",
    "    if \"data science\" in status_update[\"text\"].lower():\n",
    "        day_of_week=status_update[\"created_at\"].weekday()\n",
    "        yield (day_of_week,1)\n",
    "\n",
    "data_science_days=map_reduce(status_updates,data_science_day_mapper,sum_reducer)\n",
    "\n",
    "\n",
    "\n",
    "def words_per_user_mapper(status_update):\n",
    "    user=status_update[\"username\"]\n",
    "    for word in tokenize(status_update[\"text\"]):\n",
    "        yield (user,(word,1))\n",
    "\n",
    "def most_popular_word_reducer(user,words_and_counts):\n",
    "    word_counts=Counter()\n",
    "    for word,count in words_and_counts:\n",
    "        word_counts[word]+=count\n",
    "    word,count=word_counts.most_common(1)[0]\n",
    "    \n",
    "    yield(user,(word,count))\n",
    "\n",
    "user_words=map_reduce(status_updates,words_per_user_mapper,most_popular_word_reducer)\n",
    "\n",
    "def like_mapper(status_update):\n",
    "    user=status_update[\"username\"]\n",
    "    for liker in status_update[\"liked_by\"]:\n",
    "        yield(user,liker)\n",
    "\n",
    "distinct_likers_per_user=map_reduce(status_updates,liker_mapper,count_distinct_reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_multiply_mapper(m,element):\n",
    "    name,i,j,value=element\n",
    "    \n",
    "    if name==\"A\":\n",
    "        for k in range(m):\n",
    "            yield((i,k),(j,value))\n",
    "    else:\n",
    "        for k in range(m):\n",
    "            yield((k,j),(i,value))\n",
    "\n",
    "def matrix_multiply_reducer(m,key,indexed_values):\n",
    "    results_by_index=defaultdict(list)\n",
    "    for index,value in indexed_values:\n",
    "        results_by_index[index].append(value)\n",
    "        \n",
    "    sum_product=sum(results[0]*results[1]\n",
    "                   for results in results_by_index.values()\n",
    "                   if len(results)==2)\n",
    "    if sum_product !=0.0:\n",
    "        yield(key,sum_product)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
